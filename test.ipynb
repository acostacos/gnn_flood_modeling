{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from models import GAT, GCN, SWEGNN\n",
    "from data import TemporalGraphDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e2dad36fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "dataset, info = TemporalGraphDataset(node_features=config['node_features'],\n",
    "                    edge_features=config['edge_features'],\n",
    "                    **config['dataset_parameters']).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[1268, 6], edge_index=[2, 2612], edge_attr=[2612, 8], y=[1268, 1], pos=[2, 1268])\n",
      "<class 'torch.Tensor'> torch.Size([1268, 6])\n",
      "<class 'torch.Tensor'> torch.Size([2, 2612])\n",
      "<class 'torch.Tensor'> torch.Size([2612, 8])\n",
      "<class 'torch.Tensor'> torch.Size([1268, 1])\n",
      "{'num_static_node_features': 3, 'num_dynamic_node_features': 1, 'num_static_edge_features': 5, 'num_dynamic_edge_features': 1, 'previous_timesteps': 2}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(type(dataset[0].x), dataset[0].x.shape)\n",
    "print(type(dataset[0].edge_index), dataset[0].edge_index.shape)\n",
    "print(type(dataset[0].edge_attr), dataset[0].edge_attr.shape)\n",
    "print(type(dataset[0].y), dataset[0].y.shape)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(len(dataset) * 0.8) # 80% train, 20% test\n",
    "\n",
    "train_dataset = dataset[:num_train]\n",
    "# train_loader = DataLoader(train_dataset) # batch_size=32, shuffle=True\n",
    "\n",
    "test_dataset = dataset[num_train:]\n",
    "# test_loader = DataLoader(test_dataset) # batch_size=32, shuffle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "base_model_params = {\n",
    "    'static_node_features': info['num_static_node_features'],\n",
    "    'dynamic_node_features': info['num_dynamic_node_features'],\n",
    "    'static_edge_features': info['num_static_edge_features'],\n",
    "    'dynamic_edge_features': info['num_dynamic_edge_features'],\n",
    "    'previous_timesteps': info['previous_timesteps'],\n",
    "    'device': device,\n",
    "}\n",
    "lr_info = config['training_parameters']\n",
    "model_info = config['model_parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_func, optimizer):\n",
    "    start_time = time.time()\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for graph in train_dataset:\n",
    "            graph = graph.to(device)\n",
    "            labels = graph.y\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(graph)\n",
    "\n",
    "            loss = loss_func(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss = running_loss / num_train\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {epoch_loss:.4f}')\n",
    "    end_time = time.time()\n",
    "    print(f'Total training time: {end_time - start_time} seconds')\n",
    "\n",
    "\n",
    "def test(model, loss_func):\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for graph in test_dataset:\n",
    "            graph = graph.to(device)\n",
    "            labels = graph.y\n",
    "\n",
    "            outputs = model(graph)\n",
    "\n",
    "            loss = loss_func(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Print validation statistics\n",
    "    print(f'Validation Loss: {running_loss:.4f}')\n",
    "    print(f'Inference time: {end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 5.9578\n",
      "Epoch [2/10], Training Loss: 0.0264\n",
      "Epoch [3/10], Training Loss: 0.0226\n",
      "Epoch [4/10], Training Loss: 0.0208\n",
      "Epoch [5/10], Training Loss: 0.0206\n",
      "Epoch [6/10], Training Loss: 0.0206\n",
      "Epoch [7/10], Training Loss: 0.0206\n",
      "Epoch [8/10], Training Loss: 0.0206\n",
      "Epoch [9/10], Training Loss: 0.0206\n",
      "Epoch [10/10], Training Loss: 81.5901\n",
      "Total training time: 15.083606243133545 seconds\n"
     ]
    }
   ],
   "source": [
    "gcn_params = model_info['GCN']\n",
    "model = GCN(**gcn_params, **base_model_params)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_info['learning_rate'], weight_decay=lr_info['weight_decay'])\n",
    "loss_func = torch.nn.L1Loss()\n",
    "\n",
    "train(model, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 27.0272\n",
      "Inference time: 0.17727231979370117 seconds\n"
     ]
    }
   ],
   "source": [
    "test(model, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 5.4834\n",
      "Epoch [2/10], Training Loss: 0.0218\n",
      "Epoch [3/10], Training Loss: 0.0214\n",
      "Epoch [4/10], Training Loss: 0.0220\n",
      "Epoch [5/10], Training Loss: 0.0609\n",
      "Epoch [6/10], Training Loss: 0.0226\n",
      "Epoch [7/10], Training Loss: 0.0207\n",
      "Epoch [8/10], Training Loss: 17.3843\n",
      "Epoch [9/10], Training Loss: 0.2914\n",
      "Epoch [10/10], Training Loss: 0.0240\n",
      "Total training time: 21.568446159362793 seconds\n"
     ]
    }
   ],
   "source": [
    "gat_params = model_info['GAT']\n",
    "model = GAT(**gat_params, **base_model_params)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_info['learning_rate'], weight_decay=lr_info['weight_decay'])\n",
    "loss_func = torch.nn.L1Loss()\n",
    "\n",
    "train(model, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7756\n",
      "Inference time: 0.2318413257598877 seconds\n"
     ]
    }
   ],
   "source": [
    "test(model, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.0733\n",
      "Epoch [2/10], Training Loss: 0.0218\n",
      "Epoch [3/10], Training Loss: 0.0950\n",
      "Epoch [4/10], Training Loss: 0.0206\n",
      "Epoch [5/10], Training Loss: 0.0220\n",
      "Epoch [6/10], Training Loss: 0.0206\n",
      "Epoch [7/10], Training Loss: 0.0206\n",
      "Epoch [8/10], Training Loss: 0.0206\n",
      "Epoch [9/10], Training Loss: 0.0208\n",
      "Epoch [10/10], Training Loss: 0.0207\n",
      "Total training time: 173.00833797454834 seconds\n"
     ]
    }
   ],
   "source": [
    "swe_gnn_params = model_info['SWEGNN']\n",
    "model = SWEGNN(**swe_gnn_params, **base_model_params)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_info['learning_rate'], weight_decay=lr_info['weight_decay'])\n",
    "loss_func = torch.nn.L1Loss()\n",
    "\n",
    "train(model, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1828\n",
      "Inference time: 1.2645950317382812 seconds\n"
     ]
    }
   ],
   "source": [
    "test(model, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.6731\n",
      "Epoch [2/10], Training Loss: 0.1494\n",
      "Epoch [3/10], Training Loss: 0.1198\n",
      "Epoch [4/10], Training Loss: 0.1002\n",
      "Epoch [5/10], Training Loss: 0.0810\n",
      "Epoch [6/10], Training Loss: 0.0594\n",
      "Epoch [7/10], Training Loss: 0.0258\n",
      "Epoch [8/10], Training Loss: 0.0306\n",
      "Epoch [9/10], Training Loss: 0.0224\n",
      "Epoch [10/10], Training Loss: 0.0214\n",
      "Total training time: 132.33159112930298 seconds\n"
     ]
    }
   ],
   "source": [
    "# No encoder decoder\n",
    "swe_gnn_params = model_info['SWEGNN']\n",
    "swe_gnn_params['encoder_layers'] = 0\n",
    "swe_gnn_params['encoder_activation'] = None\n",
    "swe_gnn_params['decoder_layers'] = 0\n",
    "swe_gnn_params['decoder_activation'] = None\n",
    "\n",
    "model = SWEGNN(**swe_gnn_params, **base_model_params)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_info['learning_rate'], weight_decay=lr_info['weight_decay'])\n",
    "loss_func = torch.nn.L1Loss()\n",
    "\n",
    "train(model, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1883\n",
      "Inference time: 1.1571977138519287 seconds\n"
     ]
    }
   ],
   "source": [
    "test(model, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Supervised Learning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from models.graph_mae2 import GraphMAE2\n",
    "from utils.graph_mae2_utils import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(model, dataset, optimizer):\n",
    "    start_time = time.time()\n",
    "\n",
    "    max_epoch = 20\n",
    "    epoch_iter = tqdm(range(max_epoch))\n",
    "    model.to(device)\n",
    "    for epoch in epoch_iter:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for graph in dataset:\n",
    "            graph = graph.to(device)\n",
    "            x = graph.x # Target\n",
    "            target_nodes = torch.arange(x.shape[0], device=device, dtype=torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model(graph, x, targets=target_nodes)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss = running_loss / num_train\n",
    "        epoch_iter.set_description(f\"# Epoch {epoch}: train_loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f'Total pre-training time: {end_time - start_time} seconds')\n",
    "    return model\n",
    "\n",
    "def linear_probing(model, dataset, in_dim, out_dim, lr_f, weight_decay_f):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Should we freeze model parameters or fine-tune them?\n",
    "    encoder = LinearRegression(in_dim, out_dim).to(device)\n",
    "\n",
    "    num_finetune_params = [p.numel() for p in encoder.parameters() if  p.requires_grad]\n",
    "    print(f\"num parameters for finetuning: {sum(num_finetune_params)}\")\n",
    "\n",
    "    loss_f = torch.nn.MSELoss()\n",
    "    optimizer_f = torch.optim.Adam(encoder.parameters(), lr=lr_f, weight_decay=weight_decay_f)\n",
    "\n",
    "    best_model = None\n",
    "    max_epoch_f = 20\n",
    "    epoch_iter_f = tqdm(range(max_epoch_f))\n",
    "    model.eval()\n",
    "    encoder.train()\n",
    "    for epoch in epoch_iter_f:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for graph in dataset:\n",
    "            optimizer_f.zero_grad()\n",
    "\n",
    "            graph = graph.to(device)\n",
    "            with torch.no_grad():\n",
    "                x = model.embed(graph)\n",
    "                x = x.to(device)\n",
    "            label = graph.y\n",
    "\n",
    "            out = encoder(x)\n",
    "            loss = loss_f(out, label)\n",
    "\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3)\n",
    "            optimizer_f.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / num_train\n",
    "        epoch_iter_f.set_description(f\"# Epoch {epoch}: train_loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print('Final loss: ', epoch_loss)\n",
    "    print(f'Total fine-tuning time: {end_time - start_time} seconds')\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Use sce_loss and alpha_l=3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Epoch 19: train_loss: 0.0903: 100%|██████████| 20/20 [02:10<00:00,  6.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pre-training time: 130.16157507896423 seconds\n",
      "num parameters for finetuning: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Epoch 19: train_loss: 29337.2740: 100%|██████████| 20/20 [00:27<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss:  29337.27402439402\n",
      "Total fine-tuning time: 27.372410535812378 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "graphmae2_params = model_info['GRAPHMAE2']\n",
    "in_dim = dataset[0].x.shape[1]\n",
    "model = GraphMAE2(in_dim=in_dim, **graphmae2_params)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_info['learning_rate'], weight_decay=lr_info['weight_decay'])\n",
    "\n",
    "trained_model = pretrain(model, train_dataset, optimizer)\n",
    "\n",
    "out_dim = dataset[0].y.shape[1]\n",
    "hidden_dim = graphmae2_params['num_hidden'] // graphmae2_params['nhead']\n",
    "linear_probing(trained_model, train_dataset, hidden_dim, out_dim, lr_info['learning_rate'], lr_info['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
